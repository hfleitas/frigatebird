//Steps
1. Connect to the DM 
2. Run .show ingestion resources. In this there will be 2 storage accounts under TempStorage.
3. Pick the TempStorage that matches the .show ingestion failures of the engine node source blobs. The URL from the DM resources contains a SAS key at the end i.e. ?sv....
4. Copy the SAS, attach it at the end of the blob needed to review out of the .show ingestion failures of the engine nodes. ie. https://<account>.blob.core...net/<date>-ingestdata-<id>-0<databasename>__<table>__StreamUpload__<guid>.json.gz?sv.....xxxxxx
5. On a browser paste the concatenated URL to download the blob and open it with VSCode or text editor to review if valid or malformed.
6. Test ingesting the blob via Get Data UI to the existing table and using the existing mapping.
7. Query the table to review manual attempt. Consider editing the shape of the data in the blob and/or altering the table.column policy encoding to type BigObject32.


#connect cluster('https://ingest-kvc43f0ee6600e24ef2b0e.southcentralus.kusto.windows.net')

.show ingestion resources | where * == 'TempStorage'


#connect cluster('https://kvc43f0ee6600e24ef2b0e.southcentralus.kusto.windows.net')

.show ingestion failures | where Table =='MyTableRaw' | sort by FailedOn | top 1 by FailedOn

// https://<account>.blob.core...net/<date>-ingestdata-<id>-0<databasename>__<table>__StreamUpload__<guid>.json.gz?sv.....xxxxxx

// click Get Data attempt manually upload

MyTableRaw
| take 10

.alter column MyTableRaw.Records policy encoding type='BigObject32'


// repeat Get Data attempt

MyTableRaw
| take 10
| extend ingestion_time()

// delete records by ingestion_time() 

// fix ingestor agent as needed.
